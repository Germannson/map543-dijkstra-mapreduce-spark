{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for local use\n",
    "#in case problem with PYTHONHASHSEED occurs on cluster see https://www.thoughtvector.io/blog/python-3-on-spark-return-of-the-pythonhashseed/\n",
    "#don't forget to unset on local env afterwards\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName(\"Dijkstra\").set(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "sc = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(path_to_file):\n",
    "    \"\"\" read graph in the moodle\"\"\"\n",
    "    res = sc.textFile(path_to_file).map(lambda x : x.split(\"\\t\"))\n",
    "    res = res.flatMap(lambda x : [(x[0], (x[1], int(x[2]))), (x[1], (x[0], int(x[3])))])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_twitter_graph(path_to_file):\n",
    "    \"\"\" read the twitter graph\"\"\"\n",
    "    res = sc.textFile(path_to_file).map(lambda x : tuple(x.split(\" \")))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_generated_graph_line(line):\n",
    "    line = line.strip().split(\"\\t\")\n",
    "    if len(line) == 2:\n",
    "        return\n",
    "    elif len(line) == 3:\n",
    "        origin = line[0]\n",
    "        neighbours = line[2]\n",
    "        try:\n",
    "            return [(origin, (pair.split(\":\")[0].strip(), int(pair.split(\":\")[1].strip())))\n",
    "                    for pair in neighbours.split(\",\")]\n",
    "        except IndexError:\n",
    "            raise RuntimeError(\"file not well formatted 1\")\n",
    "    else:\n",
    "        raise RuntimeError(\"file not well formatted 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph non pondéré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very small graph\n",
    "directions = sc.parallelize([(1,2),(1,3),(2,4),(3,5),(4,5)])\n",
    "begin = 1\n",
    "end = 5\n",
    "shortest_paths = sc.parallelize([(begin, (0,[]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = read_twitter_graph(\"twitter_combined.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin, end = '15519274', '309366491'\n",
    "shortest_paths = sc.parallelize([(begin, (0,[]))])\n",
    "early_stop = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mnt/spark/twitter_combined.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)\n\tat org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:84)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, numPartitions)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m         \"\"\"\n\u001b[0;32m-> 1676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpython_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mleftOuterJoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/join.py\u001b[0m in \u001b[0;36mpython_join\u001b[0;34m(rdd, other, numPartitions)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mwbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvbuf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_do_python_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/join.py\u001b[0m in \u001b[0;36m_do_python_join\u001b[0;34m(rdd, other, numPartitions, dispatch)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    578\u001b[0m                       self.ctx.serializer)\n\u001b[1;32m    579\u001b[0m         if (self.partitioner == other.partitioner and\n\u001b[0;32m--> 580\u001b[0;31m                 self.getNumPartitions() == rdd.getNumPartitions()):\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \"\"\"\n\u001b[0;32m--> 401\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/mnt/spark/twitter_combined.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:53)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)\n\tat org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:84)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:84)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "while True:\n",
    "    res = shortest_paths.lookup(end)\n",
    "    i+=1\n",
    "    print(i)\n",
    "    if not (res == [] and i < early_stop):\n",
    "        break\n",
    "\n",
    "    shortest_paths = shortest_paths.join(directions).map(lambda x : (x[1][1], (x[1][0][0] + 1 , x[1][0][1] + [x[0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## graph pondéré"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V1 (brute force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connections are denoted (origin, (end, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_path(x):\n",
    "    #x is the result of the join operation\n",
    "    # the join should be in format (origin, ((weight_to_origin, path_to_origin, paths_visited_to_origin), (destination, weight_to_destination)))\n",
    "    return (x[1][1][0], (x[1][0][0] + x[1][1][1], x[1][0][1] + [x[0]], {x[0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_to_point(x,y):\n",
    "    if x[0] <= y[0]:\n",
    "        res = (x[0], x[1], x[2] | y[2])\n",
    "    else:\n",
    "        res = (y[0], y[1], x[2] | y[2])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for a small graph test\n",
    "directions = sc.parallelize([(1,(2,1)),(1,(3, 5)),(3,(2,2)),(2,(4,2)),(3,(5,2)),(4,(5,2))])\n",
    "begin = 1\n",
    "objective = 5\n",
    "paths_to_objective = set(directions.map(lambda x : (x[1][0] , x[0])).filter(lambda x : x[0] == objective).values().collect())\n",
    "shortest_paths = sc.parallelize([(begin, (0,[],set()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "directions = read_generated_graph(\"../graph-data/weighted-graph-100.txt\")\n",
    "begin, objective = directions.keys().takeSample(False, 2)\n",
    "paths_to_objective = set(directions.map(lambda x : (x[1][0] , x[0])).filter(lambda x : x[0] == objective).values().collect())\n",
    "shortest_paths = sc.parallelize([(begin, (0,[],set()))])\n",
    "early_stop = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[]\n",
      "[]\n",
      "2\n",
      "[]\n",
      "[]\n",
      "3\n",
      "[('62', (21, ['47', '12'], {'12'}))]\n",
      "[]\n",
      "4\n",
      "[('62', (21, ['47', '12'], {'12'}))]\n",
      "[]\n",
      "5\n",
      "[('62', (21, ['47', '12'], {'12', '90'}))]\n",
      "[]\n",
      "6\n",
      "[('62', (21, ['47', '12'], {'16', '12', '90'}))]\n",
      "[]\n",
      "CPU times: user 310 ms, sys: 150 ms, total: 460 ms\n",
      "Wall time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "while True:\n",
    "    #stopping criteria\n",
    "    i += 1\n",
    "    print(i)\n",
    "    if i > early_stop :\n",
    "        break\n",
    "    path_found = shortest_paths.filter(lambda x : x[0] == objective).collect()\n",
    "    print(path_found)\n",
    "    print(shortest_paths.lookup(objective))\n",
    "    if path_found != [] and path_found[0][1][2] == paths_to_objective :\n",
    "        break\n",
    "        \n",
    "    #real thing\n",
    "    shortest_paths = shortest_paths.join(directions).map(compute_path).union(shortest_paths)\n",
    "    shortest_paths = shortest_paths.reduceByKey(shortest_path_to_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('62', (21, ['47', '12'], {'12', '16', '90'}))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_paths.filter(lambda x : x[0] == objective).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('47', '62')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin, objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_path(x):\n",
    "    #x is the result of the join operation\n",
    "    # the join should be in format (origin, ((weight_to_origin, path_to_origin, paths_visited_to_origin), (destination, weight_to_destination)))\n",
    "    return (x[1][1][0], {\n",
    "                         \"weight_of_path\" : x[1][0][\"weight_of_path\"] + x[1][1][1], \n",
    "                         \"path\" : x[1][0][\"path\"] + [x[0]]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_to_point(x,y):\n",
    "    if x[\"weight_of_path\"] <= y[\"weight_of_path\"]:\n",
    "        res = {\"weight_of_path\" : x[\"weight_of_path\"], \n",
    "               \"path\" : x[\"path\"]}\n",
    "    else:\n",
    "        res = {\"weight_of_path\" : y[\"weight_of_path\"], \n",
    "               \"path\" : y[\"path\"]}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for small graph\n",
    "begin = 1\n",
    "objective = 5\n",
    "early_stop = 1\n",
    "directions = sc.parallelize([(1,(2,1)),(1,(3, 5)),(3,(2,2)),(2,(4,2)),(3,(5,2)),(4,(5,2))])\n",
    "paths_to_objective = set(directions.map(lambda x : (x[1][0] , x[0])).lookup(5))\n",
    "shortest_paths = sc.parallelize([(begin, {\"weight_of_path\" :0, \"path\" : [], \"explored_path\" : set()})])\n",
    "points_to_drop = sc.broadcast(set())\n",
    "continue_criteria = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_part = 10\n",
    "early_stop = 40\n",
    "final_paths = sc.emptyRDD()\n",
    "continue_criteria = True\n",
    "points_to_drop = sc.broadcast(set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1418] at parallelize at PythonRDD.scala:175"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directions = sc.textFile(\"../graph-data/weighted-graph-100.txt\").flatMap(read_generated_graph_line)\n",
    "begin = \"1\"\n",
    "shortest_paths = sc.parallelize([(begin, {\"weight_of_path\" :0, \"path\" : []})])\n",
    "shortest_paths.partitionBy(n_part)\n",
    "shortest_paths.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### iteration 0 ######\n",
      "size directions : 228\n",
      "size paths 1\n",
      "join result size : 3\n",
      "join time : 0.9237034320831299\n",
      "min time = 0.33176612854003906\n",
      "find points to drop : 0.09117341041564941\n",
      "reduce by key : 1.0382962226867676\n",
      "filter directions : 0.00021338462829589844\n",
      "total_time : 2.3866891860961914 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 1 ######\n",
      "size directions : 224\n",
      "size paths 3\n",
      "join result size : 9\n",
      "join time : 1.257420301437378\n",
      "min time = 0.3612027168273926\n",
      "find points to drop : 0.387911319732666\n",
      "reduce by key : 1.4891681671142578\n",
      "filter directions : 0.001260995864868164\n",
      "total_time : 3.4974684715270996 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 2 ######\n",
      "size directions : 213\n",
      "size paths 7\n",
      "join result size : 16\n",
      "join time : 1.7398903369903564\n",
      "min time = 0.3234846591949463\n",
      "find points to drop : 0.6731269359588623\n",
      "reduce by key : 2.231428384780884\n",
      "filter directions : 0.00023555755615234375\n",
      "total_time : 4.968865394592285 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 3 ######\n",
      "size directions : 207\n",
      "size paths 15\n",
      "join result size : 28\n",
      "join time : 1.8686485290527344\n",
      "min time = 0.3226191997528076\n",
      "find points to drop : 1.030268669128418\n",
      "reduce by key : 2.690837860107422\n",
      "filter directions : 0.0007035732269287109\n",
      "total_time : 5.913931369781494 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 4 ######\n",
      "size directions : 191\n",
      "size paths 24\n",
      "join result size : 51\n",
      "join time : 2.3188109397888184\n",
      "min time = 0.325606107711792\n",
      "find points to drop : 1.2975187301635742\n",
      "reduce by key : 3.417266845703125\n",
      "filter directions : 8.893013000488281e-05\n",
      "total_time : 7.35965633392334 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 5 ######\n",
      "size directions : 187\n",
      "size paths 44\n",
      "join result size : 90\n",
      "join time : 2.6645476818084717\n",
      "min time = 0.3379511833190918\n",
      "find points to drop : 1.4920566082000732\n",
      "reduce by key : 3.9577691555023193\n",
      "filter directions : 0.008225679397583008\n",
      "total_time : 8.461034297943115 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 6 ######\n",
      "size directions : 177\n",
      "size paths 54\n",
      "join result size : 108\n",
      "join time : 3.0685226917266846\n",
      "min time = 0.3462657928466797\n",
      "find points to drop : 2.14882755279541\n",
      "reduce by key : 4.564856290817261\n",
      "filter directions : 0.00019931793212890625\n",
      "total_time : 10.129497051239014 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 7 ######\n",
      "size directions : 168\n",
      "size paths 57\n",
      "join result size : 114\n",
      "join time : 3.395822048187256\n",
      "min time = 0.3352680206298828\n",
      "find points to drop : 2.055764675140381\n",
      "reduce by key : 5.292477369308472\n",
      "filter directions : 0.0002887248992919922\n",
      "total_time : 11.080570220947266 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 8 ######\n",
      "size directions : 152\n",
      "size paths 58\n",
      "join result size : 105\n",
      "join time : 3.780672550201416\n",
      "min time = 0.3348250389099121\n",
      "find points to drop : 2.5360124111175537\n",
      "reduce by key : 6.003536939620972\n",
      "filter directions : 0.00010991096496582031\n",
      "total_time : 12.655532598495483 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 9 ######\n",
      "size directions : 144\n",
      "size paths 57\n",
      "join result size : 102\n",
      "join time : 4.074877500534058\n",
      "min time = 0.33226752281188965\n",
      "find points to drop : 2.7645716667175293\n",
      "reduce by key : 6.622781276702881\n",
      "filter directions : 0.0006906986236572266\n",
      "total_time : 13.795624256134033 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 10 ######\n",
      "size directions : 132\n",
      "size paths 55\n",
      "join result size : 92\n",
      "join time : 4.236201524734497\n",
      "min time = 0.31111717224121094\n",
      "find points to drop : 2.925633668899536\n",
      "reduce by key : 7.136618375778198\n",
      "filter directions : 0.0001552104949951172\n",
      "total_time : 14.610059261322021 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 11 ######\n",
      "size directions : 108\n",
      "size paths 47\n",
      "join result size : 71\n",
      "join time : 4.659999847412109\n",
      "min time = 0.33067870140075684\n",
      "find points to drop : 3.3924152851104736\n",
      "reduce by key : 7.420157432556152\n",
      "filter directions : 0.00011038780212402344\n",
      "total_time : 15.803771495819092 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 12 ######\n",
      "size directions : 84\n",
      "size paths 39\n",
      "join result size : 49\n",
      "join time : 4.986783504486084\n",
      "min time = 0.3385283946990967\n",
      "find points to drop : 3.7583673000335693\n",
      "reduce by key : 7.937376499176025\n",
      "filter directions : 0.0005755424499511719\n",
      "total_time : 17.022162437438965 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 13 ######\n",
      "size directions : 69\n",
      "size paths 34\n",
      "join result size : 37\n",
      "join time : 5.728397369384766\n",
      "min time = 0.33133888244628906\n",
      "find points to drop : 3.9782683849334717\n",
      "reduce by key : 10.514611005783081\n",
      "filter directions : 0.0009911060333251953\n",
      "total_time : 20.55414915084839 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 14 ######\n",
      "size directions : 54\n",
      "size paths 28\n",
      "join result size : 25\n",
      "join time : 5.767463684082031\n",
      "min time = 0.36635351181030273\n",
      "find points to drop : 4.634450912475586\n",
      "reduce by key : 9.246816873550415\n",
      "filter directions : 0.0029761791229248047\n",
      "total_time : 20.018831729888916 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 15 ######\n",
      "size directions : 45\n",
      "size paths 24\n",
      "join result size : 16\n",
      "join time : 6.158265590667725\n",
      "min time = 0.3291287422180176\n",
      "find points to drop : 4.492525815963745\n",
      "reduce by key : 9.892391443252563\n",
      "filter directions : 0.0002143383026123047\n",
      "total_time : 20.872894763946533 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 16 ######\n",
      "size directions : 36\n",
      "size paths 19\n",
      "join result size : 11\n",
      "join time : 6.4119322299957275\n",
      "min time = 0.34827733039855957\n",
      "find points to drop : 6.022169828414917\n",
      "reduce by key : 10.902015924453735\n",
      "filter directions : 8.320808410644531e-05\n",
      "total_time : 23.6853187084198 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 17 ######\n",
      "size directions : 24\n",
      "size paths 8\n",
      "join result size : 6\n",
      "join time : 7.397839069366455\n",
      "min time = 0.3881564140319824\n",
      "find points to drop : 5.351954221725464\n",
      "reduce by key : 11.287849426269531\n",
      "filter directions : 8.487701416015625e-05\n",
      "total_time : 24.426721811294556 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 18 ######\n",
      "size directions : 21\n",
      "size paths 5\n",
      "join result size : 3\n",
      "join time : 6.93653678894043\n",
      "min time = 0.31859612464904785\n",
      "find points to drop : 5.440463066101074\n",
      "reduce by key : 11.310883045196533\n",
      "filter directions : 0.00010204315185546875\n",
      "total_time : 24.006914854049683 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 19 ######\n",
      "size directions : 19\n",
      "size paths 4\n",
      "join result size : 2\n",
      "join time : 7.440274477005005\n",
      "min time = 0.33428454399108887\n",
      "find points to drop : 5.862915277481079\n",
      "reduce by key : 12.723995685577393\n",
      "filter directions : 0.0010886192321777344\n",
      "total_time : 26.36305832862854 \n",
      "\n",
      "\n",
      "\n",
      "##### iteration 20 ######\n",
      "size directions : 16\n",
      "size paths 2\n",
      "join result size : 0\n",
      "join time : 8.082921266555786\n",
      "reduce by key : 13.276017904281616\n",
      "filter directions : 0.0010945796966552734\n",
      "total_time : 21.678874015808105 \n",
      "\n",
      "\n",
      "\n",
      "CPU times: user 2.81 s, sys: 1.18 s, total: 3.99 s\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = 0\n",
    "while continue_criteria:\n",
    "    print(\"##### iteration {} ######\".format(i))\n",
    "    print(\"size directions : {}\".format(directions.count()))\n",
    "    print(\"size paths {}\".format(shortest_paths.count()))\n",
    "\n",
    "    time_0 = time.time()\n",
    "    time_1 = time.time()\n",
    "\n",
    "    # finding all the paths connected with the already visited points\n",
    "    new_paths = shortest_paths.join(directions, n_part).map(compute_path).cache()\n",
    "    print(\"join result size : {}\".format(new_paths.count()))\n",
    "    print(\"join time : {}\".format(time.time() - time_0))\n",
    "    time_0 = time.time()\n",
    "    try:\n",
    "        # value of the minimum path to one of those points reached at step n+1\n",
    "        min_new_paths = sc.broadcast(new_paths.map(lambda x: x[1][\"weight_of_path\"]).min())\n",
    "        print(\"min time = {}\".format(time.time() - time_0))\n",
    "        time_0 = time.time()\n",
    "\n",
    "        # we can now abandon all the paths reached at step n with a smaller path than the min calculated above\n",
    "        # (these paths cannot be improoved further)\n",
    "        points_to_drop = sc.broadcast(set(shortest_paths.filter(\n",
    "            lambda x: x[1][\"weight_of_path\"] <= min_new_paths.value).keys().collect()) | points_to_drop.value)\n",
    "        print(\"find points to drop : {}\".format(time.time() - time_0))\n",
    "    except ValueError:\n",
    "        # if no new paths are detected:\n",
    "        min_new_paths = sc.broadcast(float(\"inf\"))\n",
    "\n",
    "    # we can now combine the new paths with the reamining old paths\n",
    "    time_0 = time.time()\n",
    "    shortest_paths = new_paths.union(shortest_paths).reduceByKey(shortest_path_to_point)\n",
    "    final_paths = final_paths.union(shortest_paths.filter(lambda x: x[0] in points_to_drop.value))\n",
    "    shortest_paths = shortest_paths.filter(lambda x: x[0] not in points_to_drop.value)\n",
    "    shortest_paths.cache()\n",
    "    shortest_paths.collect()\n",
    "    print(\"reduce by key : {}\".format(time.time() - time_0))\n",
    "\n",
    "    # we can also drop all the directions going from and to the droped points in order to increase speed of the join\n",
    "    time_0 = time.time()\n",
    "    directions = directions.filter(lambda x: x[0] not in points_to_drop.value and x[1][0] not in points_to_drop.value)\n",
    "\n",
    "    # stopping criteria\n",
    "    i += 1\n",
    "\n",
    "    continue_criteria = min_new_paths.value != float(\"inf\") and i < early_stop\n",
    "    print(\"filter directions : {}\".format(time.time() - time_0))\n",
    "    print(\"total_time : {} \\n\\n\\n\".format(time.time() - time_1))\n",
    "final_paths = final_paths.union(shortest_paths)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt(path):\n",
    "    res = str(path[0])\n",
    "    res += \" : \" + \">\".join(path[1][\"path\"])\n",
    "    res += \" for weight \" + str(path[1][\"weight_of_path\"])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 :  for weight 0']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shortest_paths.map(get_txt).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
